{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abbcca1-f02a-43d8-872b-7d7a63113ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ALREADY EXISTS\n"
     ]
    }
   ],
   "source": [
    "## Inflation Forecast\n",
    "## Same as version 3, but do partially linear model\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sqlite3\n",
    "import random\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "# from functions import *\n",
    "\n",
    "# Seed number used\n",
    "seed = 42\n",
    "\n",
    "# Transform:\n",
    "price_var = ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'OILPRICEx', 'PPICMM', 'CPIAUCSL', \n",
    "             'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', \n",
    "             'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA']\n",
    "\n",
    "# Transform:\n",
    "# Transformation = 'No Transform'\n",
    "Transformation = 'Transform' \n",
    "# :Transforms according to the recommendations given by McCracken and Ng (2015) for all but Group 7 (Prices),\n",
    "#  which are transformed as year over year growth\n",
    "\n",
    "\n",
    "\n",
    "# Make Database\n",
    "con = sqlite3.connect(os.path.join('Data', 'database_predict_inflation.db'))\n",
    "cur = con.cursor()\n",
    "\n",
    "res = cur.execute(\"\"\"SELECT name FROM sqlite_master WHERE type='table'\"\"\")\n",
    "table_names = res.fetchall()\n",
    "if ~np.isin('Results', table_names):\n",
    "    print(\"CREATE NEW DATABASE\")\n",
    "    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS Results(\n",
    "                Date TEXT NOT NULL,\n",
    "                Target TEXT NOT NULL,\n",
    "                Value REAL NOT NULL,\n",
    "                Prediction REAL NOT NULL,\n",
    "                Model TEXT NOT NULL,\n",
    "                Seed INTEGER NOT NULL,\n",
    "                Parameter TEXT,\n",
    "                Window_size INTEGER NOT NULL,\n",
    "                Validation_size INTEGER NOT NULL,\n",
    "                Transformation TEXT NOT NULL,\n",
    "                PRIMARY KEY (Date, Target, Model, Seed, Window_size, Validation_size, Transformation))\"\"\")\n",
    "    con.commit()\n",
    "else:\n",
    "    print(\"DATABASE ALREADY EXISTS\")\n",
    "    con.commit()\n",
    "\n",
    "Data = pd.read_csv(os.path.join('Data', '2024-06.csv'))\n",
    "p = Data.shape[1]\n",
    "\n",
    "temp_list = []\n",
    "for i in range(1,p):\n",
    "    tcode = Data.iloc[0,i].copy()\n",
    "    data = Data.iloc[1:,i].copy()\n",
    "    if data.name == 'CPIAUCSL':\n",
    "        data_transform = np.log(data).diff(periods=12)           # Year over year growth\n",
    "    elif np.isin(data.name, list(set(price_var)-set(['CPIAUCSL']))):\n",
    "        if Transformation == 'Transform':\n",
    "            data_transform = np.log(data).diff(periods=12)       # Year over year growth\n",
    "        else:\n",
    "            data_transform = data\n",
    "    else:\n",
    "        if np.isin(Transformation, ['Transform']):\n",
    "            if tcode == 1:\n",
    "                data_transform = data\n",
    "            elif tcode == 2:  # First difference\n",
    "                data_transform = data.diff()\n",
    "            elif tcode == 3: # Second difference\n",
    "                data_transform = data.diff().diff()\n",
    "            elif tcode == 4: # Log\n",
    "                data_transform = np.log(data)\n",
    "            elif tcode == 5: #First difference of log\n",
    "                data_transform = np.log(data).diff()\n",
    "            elif tcode == 6: #Second difference of natural log\n",
    "                data_transform = np.log(data).diff().diff()\n",
    "            elif tcode == 7: # First difference of percent change\n",
    "                data_transform = data.pct_change().diff()\n",
    "        elif Transformation == 'No Transform':\n",
    "            data_transform = data\n",
    "    temp_list.append(data_transform.copy())\n",
    "Data_transform = pd.DataFrame(temp_list).T\n",
    "\n",
    "Date = Data.iloc[1:,0]\n",
    "\n",
    "Y = Data_transform['CPIAUCSL'] # Inflation\n",
    "\n",
    "num_lags = 2\n",
    "X = Data_transform\n",
    "for p in range(1, num_lags):\n",
    "    X['CPIAUCSL_lag%i' % p] = Y.shift(p)\n",
    "\n",
    "h = 1 # One step ahead forecast\n",
    "X = X.shift(h)\n",
    "\n",
    "X_used = X.iloc[12+num_lags:,:].reset_index(drop=True)\n",
    "V_label = ['CPIAUCSL']\n",
    "for p in range(1,num_lags):\n",
    "    V_label.append('CPIAUCSL_lag%i' % p)\n",
    "V_used = X_used[V_label]\n",
    "\n",
    "W_used = X_used.drop(V_label,axis=1)\n",
    "# np.sum(np.sum(np.isnan(X_used))==0)\n",
    "Y_used = Y.iloc[12+num_lags:].reset_index(drop=True)\n",
    "\n",
    "Date_used = Date.iloc[12+num_lags:].reset_index(drop=True)\n",
    "Date_used = pd.to_datetime(Date_used)\n",
    "\n",
    "# plt.plot(np.sum(~np.isnan(X_used),axis=1))\n",
    "# plt.show()\n",
    "n = X_used.shape[0]\n",
    "\n",
    "forecast_period = pd.to_datetime('2015-01-01')<=Date_used\n",
    "forecast_idx = np.where(forecast_period)[0]\n",
    "n_test = np.sum(forecast_period)\n",
    "\n",
    "validation_period = (pd.to_datetime('2005-08-01')<=Date_used) & (pd.to_datetime('2015-01-01') > Date_used)\n",
    "validation_idx = np.where(validation_period)[0]\n",
    "n_val = np.sum(validation_period)\n",
    "\n",
    "training_period = pd.to_datetime('2005-08-01')>Date_used\n",
    "training_idx = np.where(training_period)[0]\n",
    "n_train = np.sum(training_period)\n",
    "\n",
    "\n",
    "V_train = V_used.loc[training_period]\n",
    "W_train = W_used.loc[training_period,:]\n",
    "Y_train = Y_used.loc[training_period]\n",
    "\n",
    "V_val = V_used.loc[validation_period]\n",
    "W_val = W_used.loc[validation_period,:]\n",
    "Y_val = Y_used.loc[validation_period]\n",
    "\n",
    "V_test = V_used.loc[forecast_period]\n",
    "W_test = W_used.loc[forecast_period,:]\n",
    "Y_test = Y_used.loc[forecast_period]\n",
    "\n",
    "nnan_idx = np.sum(np.isnan(W_used),axis=0)==0\n",
    "W_used_nnan = W_used.loc[:,nnan_idx]\n",
    "\n",
    "W_train_nnan = W_used_nnan.loc[training_period,:]\n",
    "W_val_nnan = W_used_nnan.loc[validation_period,:]\n",
    "W_test_nnan = W_used_nnan.loc[forecast_period,:]\n",
    "\n",
    "Validation_Err = {}\n",
    "beta_tot = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87876644-9c74-4c49-9c81-5cb9e54af553",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     val_err[:, cv_i] \u001b[38;5;241m=\u001b[39m Y_val\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m-\u001b[39mY_hat\n\u001b[0;32m     31\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed time = \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m sec; \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(elapsed, \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[0;32m     34\u001b[0m min_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(val_err)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     35\u001b[0m val_err_RF \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(val_err)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "#######################   Partially Linear Random Forest  #############################\n",
    "#######################################################################################\n",
    "max_depth_list = np.append(np.arange(1,50,3),None)\n",
    "val_err = np.zeros((n_val, len(max_depth_list)))\n",
    "RFmodel_dict = {}\n",
    "beta_dict_RF = {}\n",
    "t = time.time()\n",
    "for cv_i, max_depth in enumerate(max_depth_list):\n",
    "    model1 = RandomForestRegressor(n_estimators=100, criterion='squared_error',\n",
    "                                    max_depth=max_depth,random_state=seed)\n",
    "    model2 = RandomForestRegressor(n_estimators=100, criterion='squared_error',\n",
    "                                    max_depth=max_depth,random_state=seed)\n",
    "    RFmodel_dict[cv_i] = RandomForestRegressor(n_estimators=100, criterion='squared_error',\n",
    "                                    max_depth=max_depth,random_state=seed)    \n",
    "    model1.fit(W_train_nnan, V_train)\n",
    "    \n",
    "    resi_v = V_train.values - model1.predict(W_train_nnan)\n",
    "    model2.fit(W_train_nnan, Y_train)\n",
    "    resi_y = Y_train.values - model2.predict(W_train_nnan)\n",
    "    beta = np.linalg.inv(resi_v.T @ resi_v) @ resi_v.T @ resi_y\n",
    "    \n",
    "    resi_train = Y_train.values - V_train.values @ beta\n",
    "    \n",
    "    RFmodel_dict[cv_i].fit(W_train_nnan, resi_train)\n",
    "    resi_hat = RFmodel_dict[cv_i].predict(W_val_nnan)\n",
    "    \n",
    "    Y_hat = V_val.values @ beta + resi_hat\n",
    "    beta_dict_RF[cv_i] = beta\n",
    "    val_err[:, cv_i] = Y_val.values-Y_hat\n",
    "elapsed = time.time() - t\n",
    "print('elapsed time = %0.2f sec; %s'%(elapsed, datetime.datetime.now()))\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "val_err_RF = np.mean(np.array(val_err)**2, axis=0)\n",
    "\n",
    "Validation_Err['RF'] = pd.DataFrame()\n",
    "Validation_Err['RF']['max_depth_list'] = max_depth_list\n",
    "Validation_Err['RF']['val_err'] = val_err_RF\n",
    "beta_tot['RF'] = beta_dict_RF[min_idx]\n",
    "\n",
    "temp_grid = ['None' if x==None else x for x in max_depth_list]\n",
    "plt.plot(temp_grid, val_err_RF)\n",
    "plt.xlabel('max_depth')\n",
    "plt.title('Validation Error, Random Forest, Minimum=%s'%str(temp_grid[min_idx]))\n",
    "# plt.savefig(\"Figures/RF_validation_seed%i.png\"%seed)\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Y_hat = V_test.values @ beta_dict_RF[min_idx] + RFmodel_dict[min_idx].predict(W_test_nnan)\n",
    "test_err_RF = Y_test.values - Y_hat\n",
    "RMSE_RF = np.sqrt(np.sum(test_err_RF**2)/len(test_err_RF))\n",
    "print('The RMSE of PL-RF model is %f'%RMSE_RF)\n",
    "\n",
    "RF_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat,\n",
    "        'Model': 'Random Forest-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': str(max_depth_list[min_idx]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "RF_out = pd.DataFrame.from_dict(RF_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee9217-e6b5-446f-bfac-b6f804608858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "###################################   XGBoost  ########################################\n",
    "#######################################################################################\n",
    "n_estimators_list = np.arange(1,50,3)\n",
    "val_err = np.zeros((n_val,len(n_estimators_list)))\n",
    "XGBmodel_dict = {}\n",
    "beta_dict_XGB = {}\n",
    "t = time.time()\n",
    "for cv_i, n_estimators in enumerate(n_estimators_list):\n",
    "\n",
    "    model1 = xgb.XGBRegressor(n_jobs=1, tree_method=\"exact\", n_estimators=n_estimators, random_state=seed)\n",
    "    model2 = xgb.XGBRegressor(n_jobs=1, tree_method=\"exact\", n_estimators=n_estimators, random_state=seed)\n",
    "    XGBmodel_dict[cv_i] = xgb.XGBRegressor(n_jobs=1, tree_method=\"exact\", n_estimators=n_estimators, random_state=seed)\n",
    "\n",
    "    model1.fit(W_train_nnan, V_train)\n",
    "    resi_v = V_train.values - model1.predict(W_train_nnan)\n",
    "    model2.fit(W_train_nnan, Y_train)\n",
    "    resi_y = Y_train.values - model2.predict(W_train_nnan)\n",
    "    \n",
    "    beta = np.linalg.inv(resi_v.T @ resi_v) @ (resi_v.T @ resi_y)\n",
    "    resi_train = Y_train.values - V_train.values @ beta\n",
    "    \n",
    "    XGBmodel_dict[cv_i].fit(W_train_nnan, resi_train)\n",
    "    resi_hat = XGBmodel_dict[cv_i].predict(W_val_nnan)\n",
    "    Y_hat = V_val.values @ beta + resi_hat\n",
    "    \n",
    "    beta_dict_XGB[cv_i] = beta\n",
    "    val_err[:, cv_i] = Y_val.values-Y_hat\n",
    "elapsed = time.time() - t\n",
    "print('elapsed time = %0.2f sec; %s'%(elapsed, datetime.datetime.now()))\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "val_err_XGB = np.mean(np.array(val_err)**2, axis=0)\n",
    "\n",
    "Validation_Err['XGB'] = pd.DataFrame()\n",
    "Validation_Err['XGB']['n_estimators_list'] = n_estimators_list\n",
    "Validation_Err['XGB']['val_err'] = val_err_XGB\n",
    "beta_tot['XGB'] = beta_dict_XGB[min_idx]\n",
    "\n",
    "plt.plot(n_estimators_list,val_err_XGB)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Validation Error, XGBoost, Minimum=%i'%n_estimators_list[min_idx])\n",
    "# plt.savefig(\"Figures/XGB_validation_seed%i.png\"%seed)\n",
    "# plt.close()\n",
    "# plt.show()\n",
    "\n",
    "Y_hat = V_test.values @ beta_dict_XGB[min_idx] + XGBmodel_dict[min_idx].predict(W_test_nnan)\n",
    "test_err_XGB = Y_test.values - Y_hat\n",
    "RMSE_XGB = np.sqrt(np.sum(test_err_XGB**2)/len(test_err_XGB))\n",
    "print('The RMSE of PL-XGB model is %f'%RMSE_XGB)\n",
    "\n",
    "XGB_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat,\n",
    "        'Model': 'XGBoost-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': str(n_estimators_list[min_idx]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "XGB_out = pd.DataFrame.from_dict(XGB_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f728a5f-7b78-45de-8457-97b4bd8af6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "##############################   XGBoost with subsampling #############################\n",
    "#######################################################################################\n",
    "\n",
    "num_parallel_tree = 100\n",
    "subsample = np.sqrt(W_train_nnan.shape[0])/W_train_nnan.shape[0]\n",
    "n_estimators_list = np.arange(1,50,3)\n",
    "val_err = np.zeros((n_val,len(n_estimators_list)))\n",
    "XGBmodel_dict = {}\n",
    "beta_dict_XGBs = {}\n",
    "t = time.time()\n",
    "for cv_i, n_estimators in enumerate(n_estimators_list):\n",
    "\n",
    "    model1 = xgb.XGBRegressor(n_jobs=1, tree_method=\"exact\", n_estimators=n_estimators, random_state=seed,\n",
    "                            num_parallel_tree = num_parallel_tree, subsample = subsample)\n",
    "    model2 = xgb.XGBRegressor(n_jobs=1, tree_method=\"exact\", n_estimators=n_estimators, random_state=seed,\n",
    "                            num_parallel_tree = num_parallel_tree, subsample = subsample)\n",
    "    XGBmodel_dict[cv_i] = xgb.XGBRegressor(n_jobs=1, tree_method=\"exact\", n_estimators=n_estimators, random_state=seed,\n",
    "                                            num_parallel_tree = num_parallel_tree, subsample = subsample)\n",
    "\n",
    "    model1.fit(W_train_nnan, V_train)\n",
    "    resi_v = V_train.values - model1.predict(W_train_nnan)\n",
    "    model2.fit(W_train_nnan, Y_train)\n",
    "    resi_y = Y_train.values - model2.predict(W_train_nnan)\n",
    "    beta = np.linalg.inv(resi_v.T @ resi_v) @ (resi_v.T @ resi_y)\n",
    "    resi_train = Y_train.values - V_train.values @ beta\n",
    "    \n",
    "    XGBmodel_dict[cv_i].fit(W_train_nnan, resi_train)\n",
    "    resi_hat = XGBmodel_dict[cv_i].predict(W_val_nnan)\n",
    "    Y_hat = V_val.values @ beta + resi_hat\n",
    "    \n",
    "    beta_dict_XGBs[cv_i] = beta\n",
    "    val_err[:, cv_i] = Y_val.values-Y_hat\n",
    "elapsed = time.time() - t\n",
    "print('elapsed time = %0.2f sec; %s'%(elapsed, datetime.datetime.now()))\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "val_err_XGBs = np.mean(np.array(val_err)**2, axis=0)\n",
    "\n",
    "Validation_Err['XGBs'] = pd.DataFrame()\n",
    "Validation_Err['XGBs']['n_estimators_list'] = n_estimators_list\n",
    "Validation_Err['XGBs']['val_err'] = val_err_XGBs\n",
    "beta_tot['XGBs'] = beta_dict_XGBs[min_idx]\n",
    "\n",
    "plt.plot(n_estimators_list, val_err_XGBs)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Validation Error, XGBoost with subsample, Minimum=%i'%n_estimators_list[min_idx])\n",
    "# plt.savefig(\"Figures/XGBs_validation_seed%i.png\"%seed)\n",
    "# plt.close()\n",
    "# plt.show()\n",
    "\n",
    "Y_hat = V_test.values @ beta_dict_XGBs[min_idx] + XGBmodel_dict[min_idx].predict(W_test_nnan)\n",
    "test_err_XGBs = Y_test.values - Y_hat\n",
    "RMSE_XGBs = np.sqrt(np.sum(test_err_XGBs**2)/len(test_err_XGBs))\n",
    "print('The RMSE of PL-XGBs model is %f'%RMSE_XGBs)\n",
    "\n",
    "XGBs_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat,\n",
    "        'Model': 'XGBoost-subsample-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': str(n_estimators_list[min_idx]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "XGBs_out = pd.DataFrame.from_dict(XGBs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940fb71-1c94-4c36-84dc-9f5f89e00732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "####################################   PCR    #########################################\n",
    "#######################################################################################\n",
    "W_train_stzd = (W_train_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "W_val_stzd = (W_val_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "W_test_stzd = (W_test_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)    \n",
    "\n",
    "Sigma_hat = W_train_stzd.T@W_train_stzd/n_train\n",
    "eigval, eigvec = np.linalg.eig(Sigma_hat)\n",
    "eigval = np.real(eigval)\n",
    "eigvec = np.real(eigvec)\n",
    "idx = eigval.argsort()[::-1]\n",
    "eigval_sorted = eigval[idx]\n",
    "eigvec_sorted = eigvec[:, idx]\n",
    "F_train = W_train_stzd @ eigvec_sorted\n",
    "F_val = W_val_stzd @ eigvec_sorted\n",
    "F_val.columns = F_train.columns\n",
    "F_test = W_test_stzd @ eigvec_sorted\n",
    "F_test.columns = F_test.columns\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(W_train_stzd.columns,eigvec_sorted[:,0], label='First basis')\n",
    "# ax.plot(W_train_stzd.columns,eigvec_sorted[:,1], label='Second basis')\n",
    "# ax.plot(W_train_stzd.columns,eigvec_sorted[:,2], label='Third basis')\n",
    "# plt.xticks(rotation=-45)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "nfactors_list = np.arange(1,20)\n",
    "val_err = np.zeros((n_val, len(nfactors_list)))\n",
    "nfactors = 2\n",
    "PCR_dict = {}\n",
    "beta_dict_PCR = {}\n",
    "for cv_i, nfactors in enumerate(nfactors_list):\n",
    "\n",
    "    PCR_dict[cv_i] = LinearRegression(fit_intercept=True)\n",
    "    PCR_dict[cv_i].fit(np.concatenate((V_train.values,F_train.iloc[:,:nfactors]),axis=1), Y_train)\n",
    "    \n",
    "    \n",
    "    Y_hat = PCR_dict[cv_i].predict(np.concatenate((V_val.values,F_val.iloc[:,:nfactors]),axis=1))\n",
    "    \n",
    "    beta_dict_PCR[cv_i] = PCR_dict[cv_i].coef_[:2]\n",
    "    val_err[:, cv_i] = Y_val.values-Y_hat\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "val_err_PCR = np.mean(np.array(val_err)**2, axis=0)\n",
    "\n",
    "Validation_Err['PCR'] = pd.DataFrame()\n",
    "Validation_Err['PCR']['nfactors_list'] = nfactors_list\n",
    "Validation_Err['PCR']['val_err'] = val_err_PCR\n",
    "beta_tot['PCR'] = beta_dict_PCR[min_idx]\n",
    "\n",
    "plt.plot(nfactors_list, val_err_PCR)\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.title('Validation Error, PCR,Minimum=%i'%nfactors_list[min_idx])\n",
    "plt.show()\n",
    "# plt.savefig(\"Figures/PCR_validation_seed%i.png\"%seed)\n",
    "# plt.close()\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "Y_hat = PCR_dict[min_idx].predict(np.concatenate((V_test.values,F_test.iloc[:,:nfactors_list[min_idx]]),axis=1))\n",
    "test_err_PCR = Y_test.values - Y_hat\n",
    "RMSE_PCR = np.sqrt(np.sum(test_err_PCR**2)/len(test_err_PCR))\n",
    "print('The RMSE of factor augmented regression is %f'%RMSE_PCR)\n",
    "\n",
    "PCR_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat,\n",
    "        'Model': 'PCR-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': str(nfactors_list[min_idx]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "PCR_out = pd.DataFrame.from_dict(PCR_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9656c-f49b-4413-bbe7-ceca101c80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "####################################   PCRp    #########################################\n",
    "#######################################################################################\n",
    "W_train_stzd2 = (W_train_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "W_val_stzd2 = (W_val_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "W_test_stzd2 = (W_test_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "\n",
    "W_train_stzd = W_train_stzd2[list(set(W_train_stzd2.columns).intersection(set(price_var)))]\n",
    "W_val_stzd = W_val_stzd2[list(set(W_train_stzd2.columns).intersection(set(price_var)))]\n",
    "W_test_stzd = W_test_stzd2[list(set(W_train_stzd2.columns).intersection(set(price_var)))]\n",
    "\n",
    "Sigma_hat = W_train_stzd.T@W_train_stzd/n_train\n",
    "eigval, eigvec = np.linalg.eig(Sigma_hat)\n",
    "eigval = np.real(eigval)\n",
    "eigvec = np.real(eigvec)\n",
    "idx = eigval.argsort()[::-1]\n",
    "eigval_sorted = eigval[idx]\n",
    "eigvec_sorted = eigvec[:, idx]\n",
    "F_train = W_train_stzd @ eigvec_sorted\n",
    "F_val = W_val_stzd @ eigvec_sorted\n",
    "F_val.columns = F_train.columns\n",
    "F_test = W_test_stzd @ eigvec_sorted\n",
    "F_test.columns = F_test.columns\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(W_train_stzd.columns,eigvec_sorted[:,0], label='First basis')\n",
    "# ax.plot(W_train_stzd.columns,eigvec_sorted[:,1], label='Second basis')\n",
    "# ax.plot(W_train_stzd.columns,eigvec_sorted[:,2], label='Third basis')\n",
    "# plt.xticks(rotation=-45)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "nfactors_list = np.arange(1,20)\n",
    "val_err = np.zeros((n_val, len(nfactors_list)))\n",
    "nfactors = 2\n",
    "PCRp_dict = {}\n",
    "beta_dict_PCRp = {}\n",
    "for cv_i, nfactors in enumerate(nfactors_list):\n",
    "\n",
    "    PCRp_dict[cv_i] = LinearRegression(fit_intercept=True)\n",
    "    PCRp_dict[cv_i].fit(np.concatenate((V_train.values,F_train.iloc[:,:nfactors]),axis=1), Y_train)\n",
    "    \n",
    "    Y_hat = PCRp_dict[cv_i].predict(np.concatenate((V_val.values,F_val.iloc[:,:nfactors]),axis=1))\n",
    "    \n",
    "    beta_dict_PCRp[cv_i] = PCRp_dict[cv_i].coef_[:2]\n",
    "    val_err[:, cv_i] = Y_val.values-Y_hat\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "val_err_PCRp = np.mean(np.array(val_err)**2, axis=0)\n",
    "\n",
    "Validation_Err['PCRp'] = pd.DataFrame()\n",
    "Validation_Err['PCRp']['nfactors_list'] = nfactors_list\n",
    "Validation_Err['PCRp']['val_err'] = val_err_PCRp\n",
    "beta_tot['PCRp'] = beta_dict_PCRp[min_idx]\n",
    "\n",
    "plt.plot(nfactors_list, val_err_PCRp)\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.title('Validation Error, PCRp,Minimum=%i'%nfactors_list[min_idx])\n",
    "plt.show()\n",
    "# plt.savefig(\"Figures/PCR_validation_seed%i.png\"%seed)\n",
    "# plt.close()\n",
    "# # plt.show()\n",
    "\n",
    "Y_hat = PCRp_dict[min_idx].predict(np.concatenate((V_test.values,F_test.iloc[:,:nfactors_list[min_idx]]),axis=1))\n",
    "test_err_PCRp = Y_test.values - Y_hat\n",
    "RMSE_PCRp = np.sqrt(np.sum(test_err_PCRp**2)/len(test_err_PCRp))\n",
    "print('The RMSE of factor augmented regression (only price variables) is %f'%RMSE_PCRp)\n",
    "\n",
    "PCRp_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat,\n",
    "        'Model': 'PCRp-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': str(nfactors_list[min_idx]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "PCRp_out = pd.DataFrame.from_dict(PCRp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac96e9-f960-444e-b718-79dfd4e68e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "################################# RKHS with l2 penalty ###############################\n",
    "#######################################################################################\n",
    "W_train_stzd = (W_train_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "W_val_stzd = (W_val_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "W_test_stzd = (W_test_nnan - np.mean(W_train_nnan, axis=0))/np.std(W_train_nnan, axis = 0)\n",
    "\n",
    "Sigma_hat = W_train_stzd.T@W_train_stzd/n_train\n",
    "eigval, eigvec = np.linalg.eig(Sigma_hat)\n",
    "eigval = np.real(eigval)\n",
    "eigvec = np.real(eigvec)\n",
    "idx = eigval.argsort()[::-1]\n",
    "eigval_sorted = eigval[idx]\n",
    "eigvec_sorted = eigvec[:, idx]\n",
    "\n",
    "import numba\n",
    "@numba.njit\n",
    "def get_Gram(X,n,gamma):\n",
    "    Gram_rbf = np.zeros((n,n))\n",
    "    for t in range(n):\n",
    "        Gram_rbf[t,:] = np.exp(-gamma*np.sum((X[t,:]-X)**2,1))\n",
    "    return Gram_rbf\n",
    "\n",
    "@numba.njit\n",
    "def get_Gram_test(X_train,X_test,n_train,n_test,gamma):\n",
    "    Gram_rbf = np.zeros((n_test,n_train))\n",
    "    for t in range(n_test):\n",
    "        Gram_rbf[t,:] = np.exp(-gamma*np.sum((X_test[t,:]-X_train)**2,1))\n",
    "    return Gram_rbf\n",
    "\n",
    "gamma = (1/W_test_stzd.shape[1])\n",
    "Kernel_Gram = get_Gram(W_train_stzd.values, W_train_stzd.shape[0],gamma)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(Kernel_Gram) \n",
    "\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigval_sorted = eigenvalues[idx]\n",
    "eigvec_sorted = eigenvectors[:, idx]\n",
    "\n",
    "F_train = pd.DataFrame(eigvec_sorted*eigval_sorted)\n",
    "\n",
    "K_val = get_Gram_test(W_train_stzd.values, W_val_stzd.values, n_train, n_val, gamma)\n",
    "K_test = get_Gram_test(W_train_stzd.values, W_test_stzd.values, n_train, n_test, gamma)\n",
    "F_val = pd.DataFrame(K_val@eigvec_sorted)\n",
    "F_test = pd.DataFrame(K_test@eigvec_sorted)\n",
    "\n",
    "\n",
    "nfactors_list = np.arange(1,20)\n",
    "delta_hat_dict = {}\n",
    "beta_hat_dict = {}\n",
    "nfactors_price = 8\n",
    "\n",
    "V_train_one = np.concatenate((np.ones((n_train,1)),V_train.values,),axis=1)\n",
    "F_train_proj = (np.identity(n_train)-V_train_one@np.linalg.inv(V_train_one.T@V_train_one)@V_train_one.T)@F_train\n",
    "lambda_list = np.linspace(0,50,100)\n",
    "val_err = np.zeros((n_val, len(nfactors_list), len(lambda_list)))\n",
    "for cv_i, nfactors in enumerate(nfactors_list):\n",
    "    for cv_j, lam in enumerate(lambda_list):\n",
    "        F_train_used = F_train.iloc[:,:nfactors]\n",
    "        F_train_proj_used = F_train_proj.iloc[:,:nfactors]\n",
    "        \n",
    "        delta_hat = np.linalg.inv(F_train_used.T@F_train_proj_used + lam*np.diag(eigval_sorted[:nfactors]))@(F_train_proj_used.T@Y_train)\n",
    "        # alpha_hat = eigvec_sorted[:,:nfactors]@delta_hat\n",
    "        # beta_hat = np.linalg.inv(V_train.values.T@V_train.values)@V_train.values.T@(Y_train-Kernel_Gram@alpha_hat)\n",
    "        beta_hat = np.linalg.inv(V_train_one.T@V_train_one)@V_train_one.T@(Y_train-(eigvec_sorted[:,:nfactors]*eigval_sorted[:nfactors])@delta_hat)\n",
    "        \n",
    "        Y_hat = np.concatenate((np.ones((n_val,1)),V_val.values,),axis=1)@beta_hat + F_val.iloc[:,:nfactors]@delta_hat\n",
    "        delta_hat_dict[cv_i,cv_j] = delta_hat\n",
    "        beta_hat_dict[cv_i,cv_j] = beta_hat\n",
    "        val_err[:, cv_i,cv_j] = Y_val.values-Y_hat.values\n",
    "\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "min_idx_i,min_idx_j = np.unravel_index(min_idx, np.mean(np.array(val_err)**2, axis=0).shape)\n",
    "val_err_RKHS = np.mean(np.array(val_err)**2, axis=0)\n",
    "val_err_RKHS[min_idx_i,min_idx_j]\n",
    "np.min(val_err_RKHS)\n",
    "\n",
    "Validation_Err['RKHS'] = {}\n",
    "Validation_Err['RKHS']['nfactors_list'] = nfactors_list\n",
    "Validation_Err['RKHS']['lambda_list'] = lambda_list\n",
    "Validation_Err['RKHS']['val_err'] = val_err_RKHS\n",
    "beta_tot['RKHS'] = beta_hat_dict[min_idx_i,min_idx_j][1:]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(lambda_list, val_err_RKHS[min_idx_i,:])\n",
    "plt.xlabel('alpha')\n",
    "plt.title('Validation Error, RKHS, Dim=%i, argmin=%0.2f'%(nfactors_list[min_idx_i],lambda_list[min_idx_j]))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(nfactors_list, val_err_RKHS[:,min_idx_j])\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.title('Validation Error, RKHS, alpha=%0.2f, argmin=%i'%(lambda_list[min_idx_j],nfactors_list[min_idx_i]))\n",
    "plt.show()\n",
    "\n",
    "Y_hat = np.concatenate((np.ones((n_test,1)),V_test.values),axis=1)@beta_hat_dict[min_idx_i,min_idx_j] + F_test.iloc[:,:nfactors_list[min_idx_i]]@delta_hat_dict[min_idx_i,min_idx_j]\n",
    "test_err_PCR = Y_test.values - Y_hat.values\n",
    "RMSE_RKHS = np.sqrt(np.sum(test_err_PCR**2)/len(test_err_PCR))\n",
    "\n",
    "RKHS_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat,\n",
    "        'Model': 'PCR-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': 'alpha%f_nf%i'%(lambda_list[min_idx_j],nfactors_list[min_idx_i]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "RKHS_out = pd.DataFrame.from_dict(RKHS_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31188d88-643d-4851-9b49-30c3cc240525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "###################################   Neural Net    ###################################\n",
    "#######################################################################################\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "batch_size = W_train_nnan.shape[0]\n",
    "epochs = 20\n",
    "n_node_list = np.linspace(1,37,19).astype(int)\n",
    "val_err = np.zeros((n_val, len(n_node_list)))\n",
    "model_NN_dict = {}\n",
    "beta_dict_NN = {}\n",
    "t = time.time()\n",
    "for cv_i, n_node in enumerate(n_node_list):\n",
    "\n",
    "    model1 = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(W_train_nnan.shape[1],)),\n",
    "            tf.keras.layers.Dense(n_node, activation=\"relu\"),\n",
    "            # tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(num_lags)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model1.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "    \n",
    "    model2 = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(W_train_nnan.shape[1],)),\n",
    "            tf.keras.layers.Dense(n_node, activation=\"relu\"),\n",
    "            # tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model2.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "    \n",
    "    model_NN = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(W_train_nnan.shape[1],)),\n",
    "            tf.keras.layers.Dense(n_node, activation=\"relu\"),\n",
    "            # tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ]\n",
    "    )\n",
    "    # model_NN.summary()\n",
    "    \n",
    "    model_NN.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "    \n",
    "    model1.fit(W_train_nnan, V_train, batch_size=batch_size,\n",
    "            epochs=epochs)\n",
    "    \n",
    "    resi_v = V_train.values - model1.predict(W_train_nnan)\n",
    "    model2.fit(W_train_nnan, Y_train, batch_size=batch_size,\n",
    "            epochs=epochs)\n",
    "    resi_y = Y_train.values - model2.predict(W_train_nnan).reshape(-1,)\n",
    "    beta = np.linalg.inv(resi_v.T @ resi_v) @ (resi_v.T @ resi_y)\n",
    "    resi_train = Y_train.values - V_train.values @ beta\n",
    "    \n",
    "    model_NN_dict[cv_i] = model_NN\n",
    "    model_NN_dict[cv_i].fit(W_train_nnan, resi_train, batch_size=batch_size,\n",
    "            epochs=epochs)\n",
    "    resi_hat = model_NN_dict[cv_i].predict(W_val_nnan)\n",
    "    Y_hat = V_val.values @ beta + resi_hat.reshape(-1,)\n",
    "    \n",
    "    beta_dict_NN[cv_i] = beta\n",
    "    val_err[:, cv_i] = Y_val.values-Y_hat\n",
    "elapsed = time.time() - t\n",
    "print('elapsed time = %0.2f sec; %s'%(elapsed, datetime.datetime.now()))\n",
    "\n",
    "min_idx = np.argmin(np.mean(np.array(val_err)**2, axis=0))\n",
    "val_err_NN = np.mean(np.array(val_err)**2, axis=0)\n",
    "\n",
    "Validation_Err['NN'] = pd.DataFrame()\n",
    "Validation_Err['NN']['n_node_list'] = n_node_list\n",
    "Validation_Err['NN']['val_err'] = val_err_NN\n",
    "beta_tot['NN'] = beta_dict_NN[min_idx]\n",
    "\n",
    "plt.plot(n_node_list, val_err_NN)\n",
    "plt.xlabel('Number of Nodes')\n",
    "plt.title('Validation Error, NN, Minimum=%i'%n_node_list[min_idx])\n",
    "plt.show()\n",
    "# plt.savefig(\"Figures/NN_validation_seed%i.png\"%seed)\n",
    "# plt.close()\n",
    "\n",
    "Y_hat = V_test.values @ beta_dict_NN[min_idx] + model_NN_dict[min_idx].predict(W_test_nnan).reshape(-1,)\n",
    "test_err_NN = Y_test.values - Y_hat\n",
    "RMSE_NN = np.sqrt(np.sum(test_err_NN**2)/len(test_err_NN))\n",
    "print('The RMSE of NN is %f'%RMSE_NN)\n",
    "\n",
    "NN_out = {'Date': Date_used[forecast_idx].dt.strftime(\"%m/%d/%Y\").values,\n",
    "        'Target': 'Inflation',\n",
    "        'Value': Y_test.values,\n",
    "        'Prediction': Y_hat.reshape((-1,)),\n",
    "        'Model': 'NN-PL-%ilags' %num_lags,\n",
    "        'Seed': seed,\n",
    "        'Parameter': str(n_node_list[min_idx]),\n",
    "        'Window_size': n_train,\n",
    "        'Validation_size': n_val,\n",
    "        'Transformation': Transformation\n",
    "        }\n",
    "NN_out = pd.DataFrame.from_dict(NN_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30485ee7-6b2b-43ca-b8db-ff73f081c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = np.concatenate((RF_out.values, XGB_out.values, XGBs_out.values, PCR_out.values, NN_out.values), axis=0)\n",
    "\n",
    "# query = ''' insert or replace into Results (Date,Target,Value,Prediction,Model,Seed,Parameter,Window_size,Validation_size,Transformation) values (?,?,?,?,?,?,?,?,?,?) '''\n",
    "# cur.executemany(query, out)\n",
    "# con.commit()\n",
    "# con.close()\n",
    "\n",
    "# file_name1 ='Validation_Err_PL_%ilags_seed%i_%s.pkl'%(num_lags,seed, Transformation.replace(' ','_'))\n",
    "# file_name2 ='beta_tot_PL_%ilags_seed%i_%s.pkl'%(num_lags,seed,Transformation.replace(' ','_'))\n",
    "\n",
    "# with open(os.path.join('Results', file_name1), 'wb') as outp:\n",
    "#     pickle.dump(Validation_Err, outp)\n",
    "\n",
    "# with open(os.path.join('Results', file_name2), 'wb') as outp:\n",
    "#     pickle.dump(beta_tot, outp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
